{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import requests\n",
    "# from bs4 import BeautifulSoup\n",
    "# import pandas as pd\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# from sklearn.metrics.pairwise import linear_kernel\n",
    "\n",
    "# # Dictionary of queries and corresponding URLs\n",
    "# queries = {\n",
    "#     'Industry Information': 'https://en.wikipedia.org/wiki/Canoo',\n",
    "#     'Competitors Information': 'https://craft.co/canoo/competitors',\n",
    "#     'Market Trends': 'https://www.nasdaq.com/news-and-insights/markets',\n",
    "#     'Financial Performance': 'https://investors.canoo.com/financial-information/financial-results'\n",
    "# }\n",
    "\n",
    "# # Function to scrape data from a single web link\n",
    "# def scrape_data(link):\n",
    "#     response = requests.get(link)\n",
    "#     soup = BeautifulSoup(response.text, 'html.parser')\n",
    "#     return soup.get_text()\n",
    "\n",
    "# # Function to store data in a CSV file\n",
    "# def store_data_in_csv(data, filename):\n",
    "#     df = pd.DataFrame([data], columns=['text'])\n",
    "#     df.to_csv(filename, index=False)\n",
    "\n",
    "# # Convert the text data to a vector space model\n",
    "# def convert_to_vector_database(text):\n",
    "#     vectorizer = TfidfVectorizer()\n",
    "#     vector_database = vectorizer.fit_transform([text])\n",
    "#     return vector_database, vectorizer\n",
    "\n",
    "# # Function to run queries in the vector space model\n",
    "# def run_queries(query, vector_database, vectorizer):\n",
    "#     query_vector = vectorizer.transform([query])\n",
    "#     cosine_similarities = linear_kernel(query_vector, vector_database).flatten()\n",
    "#     related_docs_indices = cosine_similarities.argsort()[:-5:-1]\n",
    "#     return related_docs_indices\n",
    "\n",
    "# # Text summarization placeholder\n",
    "# def summarize_text(text):\n",
    "#     return text[:100]\n",
    "\n",
    "# # Generate a report file\n",
    "# def generate_report(summary, filename):\n",
    "#     with open(filename, 'w') as f:\n",
    "#         f.write(summary + '\\n\\n')\n",
    "\n",
    "# # Main process\n",
    "# for query, url in queries.items():\n",
    "#     # Scrape data from the URL\n",
    "#     data = scrape_data(url)\n",
    "    \n",
    "#     # Store the scraped data in a CSV file\n",
    "#     csv_filename = f\"{query.replace(' ', '_')}.csv\"\n",
    "#     store_data_in_csv(data, csv_filename)\n",
    "    \n",
    "#     # Convert the data to a vector space model\n",
    "#     vector_database, vectorizer = convert_to_vector_database(data)\n",
    "    \n",
    "#     # Run the query in the vector space model\n",
    "#     results_indices = run_queries(query, vector_database, vectorizer)\n",
    "    \n",
    "#     # For simplicity, we just take the first result to summarize as an example\n",
    "#     if results_indices.size > 0:\n",
    "#         summary = summarize_text(data)\n",
    "#         report_filename = f\"{query.replace(' ', '_')}_report.txt\"\n",
    "#         generate_report(summary, report_filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import concurrent.futures\n",
    "# import requests\n",
    "# from bs4 import BeautifulSoup\n",
    "# import pandas as pd\n",
    "\n",
    "# # Dictionary of queries and corresponding URLs\n",
    "# queries_urls = {\n",
    "#     'Industry Information': 'https://en.wikipedia.org/wiki/Canoo',\n",
    "#     'Market Trends': 'https://www.tipranks.com/stocks/goev',\n",
    "#     'Competitors Information': 'https://en.wikipedia.org/wiki/Tesla,_Inc.',\n",
    "#     'Financial Performance': 'https://investors.canoo.com/financial-information/financial-results'\n",
    "# }\n",
    "\n",
    "# # Function to scrape data from a single web link\n",
    "# def scrape_data(link):\n",
    "#     response = requests.get(link)\n",
    "#     soup = BeautifulSoup(response.text, 'html.parser')\n",
    "#     return soup.get_text()\n",
    "\n",
    "# # Function to store data in a CSV file\n",
    "# def store_data_in_csv(data, filename):\n",
    "#     df = pd.DataFrame([data], columns=['text'])\n",
    "#     df.to_csv(filename, index=False)\n",
    "\n",
    "# # Main process\n",
    "# def main():\n",
    "#     with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "#         # Start the load operations and mark each future with its URL\n",
    "#         future_to_url = {executor.submit(scrape_data, url): topic for topic, url in queries_urls.items()}\n",
    "#         for future in concurrent.futures.as_completed(future_to_url):\n",
    "#             topic = future_to_url[future]\n",
    "#             try:\n",
    "#                 data = future.result()\n",
    "#                 csv_filename = f\"{topic.replace(' ', '_')}.csv\"\n",
    "#                 store_data_in_csv(data, csv_filename)\n",
    "#                 print(f\"Data for {topic} saved successfully.\")\n",
    "#             except Exception as e:\n",
    "#                 print(f\"Error processing {topic}: {str(e)}\")\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import concurrent.futures\n",
    "# import requests\n",
    "# from bs4 import BeautifulSoup\n",
    "# import pandas as pd\n",
    "\n",
    "# # Dictionary of queries and corresponding URLs\n",
    "# queries_urls = {\n",
    "#     'Industry Information': 'https://en.wikipedia.org/wiki/Canoo',\n",
    "#     'Market Trends': 'https://www.nasdaq.com/articles/canoo:-buy-sell-or-hold',\n",
    "#     'Competitors Information': 'https://en.wikipedia.org/wiki/Tesla,_Inc.',\n",
    "#     'Financial Performance': 'https://investors.canoo.com/financial-information/financial-results'\n",
    "# }\n",
    "\n",
    "# # Function to scrape data from a single web link\n",
    "# def scrape_data(link):\n",
    "#     response = requests.get(link)\n",
    "#     # Check if the request was successful\n",
    "#     if response.status_code == 200:\n",
    "#         return response.text\n",
    "#     else:\n",
    "#         raise Exception(f\"Request to {link} returned an error: {response.status_code}\")\n",
    "\n",
    "# # Custom function to process and structure the raw HTML data\n",
    "# def process_data(html_data, topic):\n",
    "#     soup = BeautifulSoup(html_data, 'html.parser')\n",
    "\n",
    "#     # Depending on the topic, you might extract data differently\n",
    "#     # This is a placeholder for your custom logic\n",
    "#     if topic == 'Industry Information':\n",
    "#         # Extract specific data from the HTML\n",
    "#         data = {'section': [], 'content': []}\n",
    "#         # For example, extracting sections and their content from Wikipedia\n",
    "#         for section in soup.select('h2, h3'):\n",
    "#             section_title = section.get_text().strip()\n",
    "#             content = section.find_next_sibling().get_text().strip()\n",
    "#             data['section'].append(section_title)\n",
    "#             data['content'].append(content)\n",
    "#         return pd.DataFrame(data)\n",
    "\n",
    "#     elif topic == 'Competitors Information':\n",
    "#         # Extract competitor information\n",
    "#         pass  # Implement your custom logic here\n",
    "\n",
    "#     elif topic == 'Market Trends':\n",
    "#         # Extract market trends information\n",
    "#         pass  # Implement your custom logic here\n",
    "\n",
    "#     elif topic == 'Financial Performance':\n",
    "#         # Extract financial performance data\n",
    "#         pass  # Implement your custom logic here\n",
    "\n",
    "#     else:\n",
    "#         return pd.DataFrame()  # Return an empty DataFrame for unknown topics\n",
    "\n",
    "# # Function to store DataFrame in a CSV file\n",
    "# def store_data_in_csv(df, filename):\n",
    "#     df.to_csv(filename, index=False)\n",
    "\n",
    "# # Main process\n",
    "# def main():\n",
    "#     with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "#         future_to_url = {executor.submit(scrape_data, url): topic for topic, url in queries_urls.items()}\n",
    "#         for future in concurrent.futures.as_completed(future_to_url):\n",
    "#             topic = future_to_url[future]\n",
    "#             try:\n",
    "#                 html_data = future.result()\n",
    "#                 structured_data = process_data(html_data, topic)\n",
    "#                 csv_filename = f\"{topic.replace(' ', '_')}.csv\"\n",
    "#                 store_data_in_csv(structured_data, csv_filename)\n",
    "#                 print(f\"Data for {topic} saved successfully in {csv_filename}.\")\n",
    "#             except Exception as e:\n",
    "#                 print(f\"Error processing {topic}: {str(e)}\")\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Dictionary of queries and corresponding URLs\n",
    "queries_urls = {\n",
    "    'Industry Information': 'https://en.wikipedia.org/wiki/Canoo',\n",
    "    'Competitors Information': 'https://en.wikipedia.org/wiki/Tesla,_Inc.',\n",
    "    'Financial Performance': 'https://investors.canoo.com/financial-information/financial-results',\n",
    "    'Market Trends': 'https://www.nasdaq.com/articles/canoo:-buy-sell-or-hold'\n",
    "}\n",
    "\n",
    "# Function to scrape data from a single web link\n",
    "def scrape_data(link):\n",
    "    response = requests.get(link)\n",
    "    if response.status_code == 200:\n",
    "        return response.text\n",
    "    else:\n",
    "        raise Exception(f\"Request to {link} returned an error: {response.status_code}\")\n",
    "\n",
    "# Custom function to process and structure the raw HTML data\n",
    "def process_data(html_data, topic):\n",
    "    soup = BeautifulSoup(html_data, 'html.parser')\n",
    "    paragraphs = soup.find_all('p')  # Find all paragraph tags\n",
    "    \n",
    "    # Extract paragraphs with their index\n",
    "    data = {'index': [], 'paragraph': []}\n",
    "    for index, paragraph in enumerate(paragraphs, 1):\n",
    "        # Only add paragraphs with more than a few characters\n",
    "        if len(paragraph.get_text().strip()) > 20:\n",
    "            data['index'].append(index)\n",
    "            data['paragraph'].append(paragraph.get_text().strip())\n",
    "    \n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# Function to store DataFrame in a CSV file\n",
    "def store_data_in_csv(df, filename):\n",
    "    df.to_csv(filename, index=False)\n",
    "\n",
    "# Main process\n",
    "def main():\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        future_to_url = {executor.submit(scrape_data, url): topic for topic, url in queries_urls.items()}\n",
    "        for future in concurrent.futures.as_completed(future_to_url):\n",
    "            topic = future_to_url[future]\n",
    "            try:\n",
    "                html_data = future.result()\n",
    "                structured_data = process_data(html_data, topic)\n",
    "                if not structured_data.empty:\n",
    "                    csv_filename = f\"{topic.replace(' ', '_')}.csv\"\n",
    "                    store_data_in_csv(structured_data, csv_filename)\n",
    "                    print(f\"Data for {topic} saved successfully in {csv_filename}.\")\n",
    "                else:\n",
    "                    print(f\"No data extracted for {topic}.\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {topic}: {str(e)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
